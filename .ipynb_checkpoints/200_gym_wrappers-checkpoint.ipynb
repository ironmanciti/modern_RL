{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L5wRTJvKix0l"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NOShYbFix0q"
   },
   "source": [
    "# 200. Gym Wrappers\n",
    "\n",
    "OpenAI의 `gym`을 통해 제공되는 **wrappers**를 사용하면 에이전트에 제공할 observation 및 reward 수정과 같은 기능을 환경에 추가할 수 있습니다. 강화 학습에서는 관찰을 보다 쉽게 학습할 수 있도록 사전 처리하는 것이 일반적입니다. 일반적인 예는 이미지 기반 입력을 사용할 때 모든 값이 $0$와 $255$ 사이가 아니라 $0$와 $1$ 사이인지 확인하는 것입니다. RGB 이미지에서 더 일반적입니다.   \n",
    "다른 예는 환경에서 몇 가지 observation을 제공하지만 버퍼에 이를 누적하고 에이전트에게 N개의 last observation을 제공하는 것입니다. 이는 하나의 단일 프레임이 게임 state에 대한 정보에 충분하지 않을 때 동적인 컴퓨터 게임을 처리하는일반적인 시나리오입니다.    \n",
    "또 다른 예는 에이전트가 더 쉽게 소화할 수 있도록 이미지의 픽셀을 자르거나 전처리 또는 reward 점수를 정규화하는 경우입니다.   \n",
    "\n",
    "기존 환경을 \"wrapping\"하여 무언가를 수행하는 몇 가지 논리를 추가하고 싶을 때, Gym은 Wrapper 클래스라고 하는 편리한 프레임워크를 제공합니다.\n",
    "\n",
    "`gym.Wrapper` 클래스는 강화 학습을 위한 OpenAI API에 따라 환경을 정의하는 `gym.Env` 클래스를 상속합니다. `gym.Wrapper` 클래스를 구현하려면 확장할 환경을 매개변수로 받아들이는 `__init__` 메서드를 정의해야 합니다.\n",
    "\n",
    "클래스 구조는 다음 다이어그램에 나와 있습니다.\n",
    "\n",
    "Wrapper 클래스는 Env 클래스를 상속하고  Env 클래스의 인스턴스가 \"wrapping\"됩니다. 추가 기능을 추가하려면 step() 또는 reset()과 같이 확장하려는 메서드를 재정의해야 합니다. \n",
    "\n",
    "<img src=https://hub.packtpub.com/wp-content/uploads/2018/07/image1-3-437x420.png width=300>\n",
    "\n",
    "### References\n",
    "[Extending OpenAI Gym environments with Wrappers and Monitors](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2mXUuiYAix0t"
   },
   "outputs": [],
   "source": [
    "class BasicWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # modify ...\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xelxgfxiix0u"
   },
   "outputs": [],
   "source": [
    "env = BasicWrapper(gym.make(\"CartPole-v1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX8FeBxcix0v"
   },
   "source": [
    "환경이 observation, reward 및 action을 처리하는 방식을 재정의하는 `gym.Wrapper`의 하위 클래스를 사용하여 환경의 특정 측면을 수정할 수 있습니다.\n",
    "\n",
    "다음 세 가지 클래스가 이 기능을 제공합니다.  \n",
    "\n",
    "1. `gym.ObservationWrapper`: 환경에서 반환된 observation을 수정하는 데 사용됩니다. 이렇게 하려면 환경의 `observation` 메서드를 재정의합니다. 이 메서드는 단일 매개변수(수정할 observation)를 받아서 수정된 observation을 반환합니다.\n",
    "2. `gym.RewardWrapper`: 환경에서 반환되는 reward를 수정하는 데 사용됩니다. 이렇게 하려면 환경의 `reward` 메서드를 재정의합니다. 이 메서드는 단일 매개변수(수정할 reward)를 받아서 수정된 reward를 반환합니다.\n",
    "2. `gym.ActionWrapper`: 환경에 전달된 action을 수정하는 데 사용됩니다. 이렇게 하려면 환경의 `action` 메서드를 재정의합니다. 이 메서드는 단일 매개변수(수정할 action)를 받아들여 수정된 action을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vZnVW0Oeix0v"
   },
   "outputs": [],
   "source": [
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # modify obs\n",
    "        return obs\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # modify reward\n",
    "        return reward\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        # modify act\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo89Ar_iix0w"
   },
   "source": [
    "## 구현 예제\n",
    "\n",
    "구체적으로 에이전트가 보내는 action의 흐름에 개입하여 10%의 확률로 현재 action을 임의의 action으로 교체하려는 상황을 가정해 보겠습니다. 무작위 acttion을 실행하여 에이전트가 환경을 탐색하게 합니다. 이것은 ActionWrapper 클래스를 사용하여 수행하는 쉬운 작업입니다.\n",
    "\n",
    "여기에서 부모의 __init__ 메서드를 호출하고 엡실론(임의 작업의 확률)을 저장하여 wrapper를 초기화합니다.\n",
    "\n",
    "다음은 에이전트의 action을 조정하기 위해 부모 클래스에서 재정의 하는 메서드입니다. 주사위를 굴릴 때마다 엡실론의 확률로 행동 공간에서 무작위 action을 샘플링하고 에이전트가 우리에게 보낸 action 대신 반환합니다. action_space 및 래퍼 추상화를 사용하여 gym의 모든 환경에서 작동하는 추상 코드를 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rQEJ7cFSix0y"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import TypeVar\n",
    "import random\n",
    "\n",
    "Action = TypeVar('Action')\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def action(self, action):\n",
    "        if random.random() < self.epsilon:\n",
    "            print(\"Random!\")\n",
    "            return self.env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYaD3x9Cix0z",
    "outputId": "17767d40-0f87-42a8-9a60-7610e07b34f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RandomActionWrapper<TimeLimit<OrderEnforcing<CartPoleEnv<CartPole-v0>>>>>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomActionWrapper(gym.make(\"CartPole-v0\"), epsilon=0.3)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8CA24JEix01"
   },
   "source": [
    "일반 CartPole 환경을 만들고 래퍼 생성자에 전달합니다. 여기부터는 원래 CartPole 대신 wrapper를 일반 Env 인스턴스로 사용합니다. Wrapper 클래스는 Env 클래스를 상속하고 동일한 인터페이스를 노출하므로 원하는 조합으로 래퍼를 중첩할 수 있습니다. 이것은 강력하고 우아하며 일반적인 솔루션입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvJscB3Cix02",
    "outputId": "3f057887-e846-4922-bcf1-13018409276d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00532056, -0.02914683, -0.02986253, -0.01059685], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRxDIZGuix03",
    "outputId": "d60e665f-e244-4674-96a3-fe2c7830db64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.00473762, -0.22382806, -0.03007447,  0.27251652], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0W-Uky9Eix03",
    "outputId": "6ed4050d-00be-43ea-b87f-ff13bb99ea65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random!\n",
      "Random!\n",
      "Random!\n",
      "Random!\n",
      "Reward got: 15.00\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "total_reward = 0.0\n",
    "while True:\n",
    "    obs, reward, done, _ = env.step(0)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Reward got: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNzLWUnFix04"
   },
   "source": [
    "래퍼는 출판된 논문의 전처리 기준을 충족하도록 환경이 작동하는 방식을 수정하는 데 사용할 수 있습니다. [OpenAI Baselines 구현](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)에는 원본 DQN 논문 및 후속 Deepmind 출판물에 사용된 전처리를 재현하는 래퍼가 포함되어 있습니다.\n",
    "\n",
    "아래에서 우리는 `gym.Discrete` observation space 가 있는 환경을 사용하고, 예를 들어 신경망에서 사용하기 위해 discrete states의 원-핫 인코딩으로 새로운 환경을 생성하는 래퍼를 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P68tfyldix04"
   },
   "source": [
    "### Observation Wrapper\n",
    "\n",
    "gym.spaces.Box(low, high, shape=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "h2wuAPq7ix05"
   },
   "outputs": [],
   "source": [
    "class DiscreteToBoxWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete), \\\n",
    "            \"Should only be used to wrap Discrete envs.\"\n",
    "        self.n = self.observation_space.n\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (self.n,))\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        new_obs = np.zeros(self.n)\n",
    "        new_obs[obs] = 1\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper 적용 없이 환경 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "0\n",
      "4\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "T = 10\n",
    "obs = env.reset()\n",
    "for t in range(T):\n",
    "    a = env.action_space.sample()\n",
    "    obs, r, done, info = env.step(a)\n",
    "    print(obs)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQGk4nCbix05",
    "outputId": "14774bc1-9082-4d9e-d172-e25f4dc5e3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = DiscreteToBoxWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "T = 10\n",
    "obs = env.reset()\n",
    "for t in range(T):\n",
    "    a = env.action_space.sample()\n",
    "    obs, r, done, info = env.step(a)\n",
    "    print(obs)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWK9cQm8ix05"
   },
   "source": [
    "### Going Beyond the Wrapper Class\n",
    "\n",
    "여기에 정의된 것 이상으로 래퍼의 개념을 적용하는 것이 가능합니다.\n",
    "\n",
    "심층 강화 학습의 더 복잡한 응용 프로그램에서 정책을 평가하는 것은 environment를 stepping하는 것보다 훨씬 더 오래 걸릴 수 있습니다. 이는 계산 시간의 대부분이 action을 선택하는 데 사용되므로 데이터 수집이 느려진다는 것을 의미합니다. 심층 강화 학습은 데이터 집약적이기 때문에(종종 우수한 성능을 달성하기 위해 수백만 번의 경험이 필요함) 데이터를 빠르게 획득하는 데 우선순위를 두어야 합니다.\n",
    "\n",
    "다음 클래스는 환경을 반환하는 함수를 받아서 환경의 **vectorized** 버전을 반환합니다. 기본적으로 환경의 $n$개 복사본을 생성합니다. `step` 함수는 $n$ actions의 벡터를 기대하고 $n$ 다음 상태, $n$ 보상, $n$ 완료 플래그 및 $n$ 정보의 벡터를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "r2JYeKC0ix06"
   },
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for _ in range(num_envs)]  #환경의 복사본\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yqV9MGPix06",
    "outputId": "6fdacfac-f811-4610-db45-b817b517ec3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n",
      "(128,)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "num_envs = 128\n",
    "env = VectorizedEnvWrapper(lambda: gym.make(\"CartPole-v0\"), num_envs=num_envs)\n",
    "T = 10\n",
    "observations = env.reset()\n",
    "\n",
    "for t in range(T):\n",
    "    actions = np.random.randint(env.action_space.n, size=num_envs)\n",
    "    observations, rewards, dones, infos = env.step(actions)  \n",
    "    for i in range(len(dones)):\n",
    "        if dones[i]:\n",
    "            observations[i] = env.reset_at(i)\n",
    "            \n",
    "print(observations.shape)\n",
    "print(rewards.shape)\n",
    "print(dones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "101_gym-wrappers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
