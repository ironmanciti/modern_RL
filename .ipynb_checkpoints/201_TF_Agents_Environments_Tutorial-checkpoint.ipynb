{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XljsiF6lYkuV"
   },
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h3B-YBHopJI"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9c6vCPGovOM"
   },
   "source": [
    "강화 학습(RL)의 목표는 환경과 상호 작용하여 학습하는 에이전트를 설계하는 것입니다. 표준 RL 설정에서 에이전트는 모든 시간 단계에서 관찰을 수신하고 작업을 선택합니다. 작업이 환경에 적용되고 환경은 보상과 새로운 관찰을 반환합니다. 에이전트는 반환이라고도 하는 보상 합계를 최대화하기 위한 조치를 선택하도록 정책을 훈련합니다.\n",
    "\n",
    "TF-Agents에서 환경은 Python 또는 TensorFlow로 구현될 수 있습니다. Python 환경은 일반적으로 구현, 이해 및 디버그하기가 더 쉽지만 TensorFlow 환경은 더 효율적이고 자연스러운 병렬화를 허용합니다. 가장 일반적인 워크플로는 Python으로 환경을 구현하고 래퍼 중 하나를 사용하여 자동으로 TensorFlow로 변환하는 것입니다.\n",
    "\n",
    "먼저 Python 환경을 살펴보겠습니다. TensorFlow 환경은 매우 유사한 API를 따릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_16bQF0anmE"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qax00bg2a4Jj"
   },
   "source": [
    "아직 tf-agents 또는 gym을 설치하지 않았다면 다음을 실행하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKU2iY_7at8Y",
    "outputId": "c12b64cf-210e-480d-85c9-bd7d4786dc8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 29.9 MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20 kB 30.9 MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 30 kB 19.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 51 kB 7.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 61 kB 7.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 71 kB 6.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 81 kB 7.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 92 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 102 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 112 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 122 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 133 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 143 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 153 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 163 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 174 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 184 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 194 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 204 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 215 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 225 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 235 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 245 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 256 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 266 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 276 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 286 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 296 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 307 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 317 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 327 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 337 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 348 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 358 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 368 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 378 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 389 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 399 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 409 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 419 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 430 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 440 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 450 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 460 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 471 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 481 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 491 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 501 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 512 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 522 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 532 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 542 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 552 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 563 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 573 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 583 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 593 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 604 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 614 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 624 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 634 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 645 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 655 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 665 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 675 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 686 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 696 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 706 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 716 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 727 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 737 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 747 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 757 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 768 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 778 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 788 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 798 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 808 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 819 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 829 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 839 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 849 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 860 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 870 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 880 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 890 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 901 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 911 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 921 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 931 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 942 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 952 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 962 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 972 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 983 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 993 kB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.0 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.0 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.0 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 1.0 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.0 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 1.1 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.3 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.3 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.3 MB 6.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.3 MB 6.8 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1ZAoFNwnRbKK"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-y4p9i9UURn"
   },
   "source": [
    "## Python Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPSwHONKMNv9"
   },
   "source": [
    "파이썬 환경은 step(action) -> next_time_step method로 환경에 action을 적용하고, 다음 단계에 대한 다음의 정보를 반환:\n",
    "\n",
    "- observation, reward, discount     \n",
    "- step_type : 환경과의 상호 작용은 일반적으로 순서 / 에피소드의 일부입니다. 예를 들어 체스 게임에서 여러 움직임은 FIRST , MID 혹은 LAST step_type 하나가 될 수 있습니다. \n",
    "\n",
    "이 것들은 named tuple TimeStep(step_type, reward, discount, observation)로 그룹화됩니다.\n",
    "\n",
    "모든 파이썬 환경은 `environments/py_environment.PyEnvironment` 에 인터페이스를 구현해야 합니다. 주요 method는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GlD2Dd2vUTtg"
   },
   "outputs": [],
   "source": [
    "class PyEnvironment(object):\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "    self._current_time_step = self._reset()\n",
    "    return self._current_time_step\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\"\n",
    "    if self._current_time_step is None:\n",
    "        return self.reset()\n",
    "    self._current_time_step = self._step(action)\n",
    "    return self._current_time_step\n",
    "\n",
    "  def current_time_step(self):\n",
    "    return self._current_time_step\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Return time_step_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Return observation_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def action_spec(self):\n",
    "    \"\"\"Return action_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfF8koryiGPR"
   },
   "source": [
    "- `step()` 메서드 외에도 환경에서는 새 시퀀스를 시작하고 초기 `TimeStep`을 제공하는 `reset()` 메서드도 제공합니다. `reset` 메소드를 명시적으로 호출할 필요는 없습니다. 에피소드가 끝나거나 step()이 처음 호출될 때 환경이 자동으로 재설정된다고 가정합니다.\n",
    "\n",
    "- 서브클래스는 `step()` 또는 `reset()`을 직접 구현하지 않습니다. 대신 `_step()` 및 `_reset()` 메서드를 재정의합니다. 이 메서드에서 반환된 시간 단계는 캐시되어 `current_time_step()`을 통해 노출됩니다.\n",
    "\n",
    "- `observation_spec` 및 `action_spec` 메서드는 각각 관찰 및 작업의 이름, 모양, 데이터 유형 및 범위를 설명하는 `(Bounded)ArraySpecs`의 nest를 반환합니다.\n",
    "\n",
    "- TF-Agents에서 우리는 목록, 튜플, 명명된 튜플 또는 사전으로 구성된 트리와 같은 구조로 정의되는 중첩을 반복적으로 참조합니다. 관찰과 행동의 구조를 유지하기 위해 임의로 구성할 수 있습니다. 우리는 이것이 많은 관찰과 행동이 있는 보다 복잡한 환경에 매우 유용하다는 것을 발견했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r63R-RbjcIRw"
   },
   "source": [
    "### 표준 환경 사용\n",
    "\n",
    "TF 에이전트에는 OpenAI Gym, DeepMind-control 및 Atari와 같은 많은 표준 환경에 대한 래퍼가 내장되어 있어 `py_environment.PyEnvironment` 인터페이스를 따릅니다. 이러한 래핑된 환경은 환경 제품군을 사용하여 쉽게 로드할 수 있습니다. OpenAI gym에서 CartPole 환경을 로드하고 action과 time_step_spec을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kBPE5T-nb2-",
    "outputId": "7384dced-c6b7-4477-9aef-3f8d1cc24c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "time_step_spec.observation: BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
      "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', environment.time_step_spec().reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWXOC863Apo_"
   },
   "source": [
    "따라서 환경은 [0, 1]범위의 `int64` 유형의 action을 예상하고, observation이 길이 4의 `float32` 벡터이고 discount factor가 [0.0, 1.0] 범위의  `float32`인 'TimeSteps'를 반환합니다.  \n",
    "\n",
    "이제 전체 에피소드에 대해 고정된 액션 `(1,)`을 시도해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzIbOJ0-0y12",
    "outputId": "e21f6dbf-bc82-47a6-cb89-172814acdefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.02674365, -0.01681182,  0.00133867, -0.00828082], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.02707989,  0.1782909 ,  0.00117306, -0.30054107], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.02351407,  0.37339613, -0.00483777, -0.59285384], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.01604615,  0.56858546, -0.01669484, -0.8870567 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.00467444,  0.76393   , -0.03443598, -1.1849407 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.01060416,  0.9594813 , -0.05813479, -1.4882159 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.02979378,  1.1552612 , -0.08789911, -1.7984717 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.05289901,  1.3512497 , -0.12386854, -2.1171286 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.079924  ,  1.5473707 , -0.16621111, -2.4453804 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(0., dtype=float32),\n",
      " 'observation': array([ 0.11087142,  1.7434747 , -0.21511872, -2.784124  ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(2, dtype=int32)})\n"
     ]
    }
   ],
   "source": [
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "\n",
    "while not time_step.is_last():\n",
    "  time_step = environment.step(action)\n",
    "  print(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xAbBl4_PMtA"
   },
   "source": [
    "### 자신만의 Python Environment 만들기\n",
    "\n",
    "일반적인 사용 사례는 문제에 TF-Agents의 표준 에이전트(agents/ 참조) 중 하나를 적용하는 것입니다. 이렇게 하려면 문제를 환경으로 프레임화해야 합니다. 파이썬에서 환경을 구현하는 방법을 살펴보겠습니다.\n",
    "\n",
    "에이전트가 다음(블랙 잭에서 영감을 받은) 카드 게임을 하도록 훈련시키고 싶다고 가정해 보겠습니다.\n",
    "\n",
    "1. 게임은 1~10으로 번호가 매겨진 무한 카드 덱을 사용하여 진행됩니다.\n",
    "2. 매 턴마다 에이전트는 2가지를 할 수 있습니다. 새로운 무작위 카드를 얻거나 현재 라운드를 중지합니다.\n",
    "3. 목표는 라운드가 끝날 때 진행하지 않고 가능한 한 21에 가까운 카드 합계를 얻는 것입니다.\n",
    "\n",
    "게임을 나타내는 환경은 다음과 같습니다.\n",
    "\n",
    "1. 액션: 2개의 액션이 있습니다. 액션 0: 새 카드를 받고, 액션 1: 현재 라운드를 종료합니다.\n",
    "2. 관찰: 현재 라운드에 있는 카드의 합계입니다.\n",
    "3. 보상: 목표는 진행하지 않고 가능한 한 21에 가까워지는 것이므로 라운드가 끝날 때 다음 보상을 사용하여 이를 달성할 수 있습니다.  \n",
    "\n",
    "    sum_of_cards - 21 if sum_of_cards <= 21, else -21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9HD0cDykPL6I"
   },
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action == 1:\n",
    "      self._episode_ended = True\n",
    "    elif action == 0:\n",
    "      new_card = np.random.randint(1, 11)\n",
    "      self._state += new_card\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYEwyX7QsqeX"
   },
   "source": [
    "위의 환경을 정의하는 모든 작업을 올바르게 수행했는지 확인합시다. 자신의 환경을 만들 때 생성된 observation 및 time_steps가 사양에 정의된 올바른 shape과 type을 따르는지 확인해야 합니다. 이것들은 TensorFlow 그래프를 생성하는 데 사용되며, 실수하면 디버깅하기 어려운 문제가 생길 수 있습니다.\n",
    "\n",
    "환경을 검증하기 위해 무작위 정책을 사용하여 작업을 생성하고 5개의 에피소드를 반복하여 의도한 대로 작동하는지 확인합니다. environment spec을 따르지 않는 time_step을 수신하면 오류가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Hhm-5R7spVx"
   },
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_36eM7MvkNOg"
   },
   "source": [
    "이제 환경이 의도한 대로 작동한다는 것을 알았으므로 고정된 정책을 사용하여 이 환경을 실행해 보겠습니다. 3장의 카드를 요청한 다음 라운드를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FILylafAkMEx",
    "outputId": "af03a52e-a043-413e-da02-982422f54e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([0], dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([5], dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([11], dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([18], dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(0., dtype=float32),\n",
      " 'observation': array([18], dtype=int32),\n",
      " 'reward': array(-3., dtype=float32),\n",
      " 'step_type': array(2, dtype=int32)})\n",
      "Final Reward =  -3.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(1, dtype=np.int32)\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "  time_step = environment.step(get_new_card_action)\n",
    "  print(time_step)\n",
    "  cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vBLPN3ioyGx"
   },
   "source": [
    "### Environment Wrappers\n",
    "\n",
    "environment wrapper는 Python 환경을 사용하고 환경의 수정된 버전을 반환합니다. 원래 환경과 수정된 환경은 모두 `py_environment.PyEnvironment`의 인스턴스이며 여러 래퍼를 함께 chain 할 수 있습니다.  \n",
    "\n",
    "몇 가지 일반적인 래퍼는 `environments/wrappers.py`에서 찾을 수 있습니다. 예를 들어:\n",
    "\n",
    "1. `ActionDiscretizeWrapper`: 연속 동작 공간을 이산 동작 공간으로 변환합니다.\n",
    "2. `RunStats`: 수행한 단계 수, 완료된 에피소드 수 등과 같은 환경의 실행 통계를 캡처합니다.\n",
    "3. 'TimeLimit': 정해진 step 수 후에 에피소드를 종료합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8aIybRdnFfb"
   },
   "source": [
    "#### Example 1: Action Discretize Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIaxJRUpvfyc"
   },
   "source": [
    "InvertedPendulum은 `[-2, 2]` 범위에서 연속 동작을 허용하는 PyBullet 환경입니다. 이 환경에서 DQN과 같은 discrete action agent를 훈련하려면 action space를 이산화(quantize)해야 합니다. 이것이 바로 `ActionDiscretizeWrapper`가 하는 일입니다. 래핑 전후의 `action_spec`을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJxEoZ4HoyjR",
    "outputId": "8b274a08-6a98-4980-d3e6-90714ede9a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec: BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-2.0, maximum=2.0)\n",
      "Discretized Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=4)\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('Pendulum-v0')\n",
    "print('Action Spec:', env.action_spec())\n",
    "\n",
    "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
    "print('Discretized Action Spec:', discrete_action_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njFjW8bmwTWJ"
   },
   "source": [
    "래핑된 `discrete_action_env`는 `py_environment.PyEnvironment`의 인스턴스이며 일반 Python 환경처럼 취급할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l5dwAhsP_F_"
   },
   "source": [
    "## TensorFlow Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZG39AjBkTjr"
   },
   "source": [
    "TF 환경에 대한 인터페이스는 `environments/tf_environment.TFEnvironment`에 정의되어 있으며 Python 환경과 매우 유사합니다. TF 환경은 몇 가지 면에서 Python 환경과 다릅니다.\n",
    "\n",
    "* array 대신 tensor object를 생성합니다.\n",
    "* TF 환경은 사양과 비교할 때 생성된 텐서에 batch dimension을 추가합니다.\n",
    "\n",
    "Python 환경을 TFEnvs로 변환하면 tensorflow가 작업을 병렬화할 수 있습니다. 예를 들어 환경에서 데이터를 수집하고 `replay_buffer`에 추가하는 `collect_experience_op`와 `replay_buffer`에서 읽고 에이전트를 훈련하고 TensorFlow에서 자연스럽게 병렬로 실행하는 `train_op`을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WKBDDZqKTxsL"
   },
   "outputs": [],
   "source": [
    "class TFEnvironment(object):\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
    "\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
    "\n",
    "  def action_spec(self):\n",
    "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "    return self._reset()\n",
    "\n",
    "  def current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "    return self._current_time_step()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
    "    return self._step(action)\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFkBIA92ThWf"
   },
   "source": [
    "`current_time_step()` 메서드는 현재 time_step을 반환하고 필요한 경우 환경을 초기화합니다.\n",
    "\n",
    "`reset()` 메서드는 환경에서 강제로 재설정하고 current_step을 반환합니다.\n",
    "\n",
    "`action`이 이전 `time_step`에 의존하지 않는다면 `Graph` 모드에서 `tf.control_dependency`가 필요합니다.\n",
    "\n",
    "지금은 `TFEnvironments`가 어떻게 생성되는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6wS3AaLdVLT"
   },
   "source": [
    "### 자신만의 TensorFlow Environment 만들기\n",
    "\n",
    "이것은 Python에서 환경을 만드는 것보다 더 복잡하므로 이 colab에서 다루지 않습니다. 예시는 [여기](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py)에서 볼 수 있습니다. 더 일반적인 사용 사례는 Python에서 환경을 구현하고 우리의 `TFPyEnvironment` 래퍼를 사용하여 TensorFlow에서 래핑하는 것입니다(아래 참조)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_Ny2lb-dU5R"
   },
   "source": [
    "### Wrapping a Python Environment in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv4-UcurZ8nb"
   },
   "source": [
    "`TFPyEnvironment` 래퍼를 사용하여 모든 Python 환경을 TensorFlow 환경으로 쉽게 래핑할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYerqyNfnVRL",
    "outputId": "4e34ddd3-4dfa-4048-fb5d-2e1e04b655e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3WFrnX9CNpC"
   },
   "source": [
    "spec은 이제 `(Bounded)TensorSpec` 유형입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQPvC1ARYALj"
   },
   "source": [
    "### 사용 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov7EIrk8dKUU"
   },
   "source": [
    "#### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdvFqUqbdB7u",
    "outputId": "500c025d-ed78-44e4-f83c-02fdc04f58b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01403382,  0.00015755, -0.01655632, -0.03573771]],\n",
      "      dtype=float32),\n",
      " 'reward': array([0.], dtype=float32),\n",
      " 'step_type': array([0], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01403697, -0.19472311, -0.01727107,  0.2516759 ]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1], dtype=int32)})]\n",
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01403697, -0.19472311, -0.01727107,  0.2516759 ]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01014251,  0.00064115, -0.01223755, -0.04640424]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1], dtype=int32)})]\n",
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01014251,  0.00064115, -0.01223755, -0.04640424]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.01015533, -0.1943032 , -0.01316564,  0.24239264]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1], dtype=int32)})]\n",
      "Total reward: [3.]\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# reset() creates the initial time_step after resetting the environment.\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "for i in range(num_steps):\n",
    "  action = tf.constant([i % 2])\n",
    "  # applies the action and returns the new TimeStep.\n",
    "  next_time_step = tf_env.step(action)\n",
    "  transitions.append([time_step, action, next_time_step])\n",
    "  reward += next_time_step.reward\n",
    "  time_step = next_time_step\n",
    "\n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWs48LNsdLnc"
   },
   "source": [
    "#### Whole Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t561kUXMk-KM",
    "outputId": "cefd42d0-00b2-403c-b116-b08a4a5ed7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_episodes: 5 num_steps: 106\n",
      "avg_length 21.2 avg_reward: 21.2\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "101_TF_Agents_Environments_Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
