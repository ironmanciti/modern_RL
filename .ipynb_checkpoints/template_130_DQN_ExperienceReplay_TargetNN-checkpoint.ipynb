{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53008013",
   "metadata": {
    "id": "53008013"
   },
   "source": [
    "# 130. Deep Q-Network (DQN) \n",
    "\n",
    "## Deep Q-learning Target NN and DDQN - Deep Mind\n",
    "#### Playing Atari with Deep Reinforcement Learning - 2015.2\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*igVmhIWcLPrNnRPwx8PQSg.png\" width= 500 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b152ff",
   "metadata": {
    "id": "51b152ff"
   },
   "source": [
    "## Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8312f5",
   "metadata": {
    "id": "ea8312f5"
   },
   "source": [
    "환경을 초기화합니다.  \n",
    "\n",
    "[MountainCar-v0](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)  \n",
    "[LunarLander-v2](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)  \n",
    "[MountainCar-v0'](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dc611",
   "metadata": {
    "id": "fe3dc611"
   },
   "source": [
    "### Neural Network Architecture 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a430b",
   "metadata": {},
   "source": [
    "### Replay Memory 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4234e",
   "metadata": {},
   "source": [
    "### Replay Memory, Current / Target Network 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize replay memory D to capacity N\n",
    "\n",
    "#Initialize action-value function Q with random weights theta\n",
    "\n",
    "#Initialize target action-value function Q_hat with random weights theta_ = theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69b410",
   "metadata": {},
   "source": [
    "### DQN main algorithm 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43946237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for episode in range(1, n_episodes+1):\n",
    "    #Initialize sequence s1\n",
    "\n",
    "    for t in range(max_t):\n",
    "        #With probability e select a random action a\n",
    "        \n",
    "            #otherwise select a_t = argmax Q(s)\n",
    "\n",
    "        #Execute action a and observe reward\n",
    "\n",
    "        # Store transition in D\n",
    "\n",
    "\n",
    "        if len(D) > batch_size:\n",
    "            #Sample random minibatch of transitions from D\n",
    "           \n",
    "\n",
    "            # y = r if episode terminates\n",
    "\n",
    "            # otherwise y = r + gamma*maxQ_target(a')\n",
    "\n",
    "            #Perform gradient descent\n",
    "           \n",
    "\n",
    "        # Every C steps reset Q_target = Q\n",
    "       \n",
    "    \n",
    "        state = next_state\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dea2d",
   "metadata": {},
   "source": [
    "### 훈련된 network 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355d41c",
   "metadata": {
    "id": "3355d41c"
   },
   "source": [
    "### Animate it with Video"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "130_DQN_ExperienceReplay_TargetNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
