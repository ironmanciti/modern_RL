{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6D70EeAZe-Q"
   },
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aPHF9kXFggA"
   },
   "source": [
    "## 소개\n",
    "\n",
    "강화 학습의 일반적인 패턴은 지정된 수의 단계 또는 에피소드에 대해 환경에서 정책을 실행하는 것입니다. 이는 예를 들어 데이터 수집, 평가 및 에이전트의 비디오 생성 중에 발생합니다.\n",
    "\n",
    "Python으로 작성하는 것이 비교적 간단하지만, `tf.while` 루프, `tf.cond` 및 `tf.control_dependencies`가 포함되므로 TensorFlow에서 작성하고 디버깅하는 것이 훨씬 더 복잡합니다. 따라서 run 루프라는 개념을 `driver`라는 클래스로 추상화하고 Python 및 TensorFlow에서 잘 테스트된 구현을 제공합니다.\n",
    "\n",
    "또한, 각 단계에서 드라이버가 발견한 데이터는 Trajectory라는 명명된 튜플에 저장되고 재현 버퍼 및 메트릭과 같은 observer 세트로 브로드캐스팅됩니다. 이 데이터에는 환경의 관찰 값, 정책에서 권장하는 행동, 획득한 보상, 현재 유형 및 다음 단계 등이 포함됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7PM1QfMZqkS"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w-Ykwl1bn4v"
   },
   "source": [
    "tf-agents 또는 gym을 아직 설치하지 않은 경우, 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnE2CgilrngG"
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-agents\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "whYNP894FSkA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V7DEcB8IeiQ"
   },
   "source": [
    "## Python Driver\n",
    "\n",
    "`PyDriver` 클래스는 Python environment, Python policy 및 각 단계에서 업데이트할 observer의 목록을 사용합니다. 기본 메서드는 `run()`이며, 다음 종료 기준 중 하나 이상이 충족될 때까지 정책의 행동을 사용하여 환경을 단계화합니다. 종료 기준은 단계 수가 `max_steps`에 도달하거나 에피소드 수가 `max_episodes`에 도달하는 것입니다.\n",
    "\n",
    "구현은 대략 다음과 같습니다.\n",
    "\n",
    "```python\n",
    "class PyDriver(object):\n",
    "\n",
    "  def __init__(self, env, policy, observers, max_steps=1, max_episodes=1):\n",
    "    self._env = env\n",
    "    self._policy = policy\n",
    "    self._observers = observers or []\n",
    "    self._max_steps = max_steps or np.inf\n",
    "    self._max_episodes = max_episodes or np.inf\n",
    "\n",
    "  def run(self, time_step, policy_state=()):\n",
    "    num_steps = 0\n",
    "    num_episodes = 0\n",
    "    while num_steps < self._max_steps and num_episodes < self._max_episodes:\n",
    "\n",
    "      # Compute an action using the policy for the given time_step\n",
    "      action_step = self._policy.action(time_step, policy_state)\n",
    "\n",
    "      # Apply the action to the environment and get the next step\n",
    "      next_time_step = self._env.step(action_step.action)\n",
    "\n",
    "      # Package information into a trajectory\n",
    "      traj = trajectory.Trajectory(\n",
    "         time_step.step_type,\n",
    "         time_step.observation,\n",
    "         action_step.action,\n",
    "         action_step.info,\n",
    "         next_time_step.step_type,\n",
    "         next_time_step.reward,\n",
    "         next_time_step.discount)\n",
    "\n",
    "      for observer in self._observers:\n",
    "        observer(traj)\n",
    "\n",
    "      # Update statistics to check termination\n",
    "      num_episodes += np.sum(traj.is_last())\n",
    "      num_steps += np.sum(~traj.is_boundary())\n",
    "\n",
    "      time_step = next_time_step\n",
    "      policy_state = action_step.state\n",
    "\n",
    "    return time_step, policy_state\n",
    "```\n",
    "\n",
    "이제 CartPole 환경에서 임의의 정책을 실행하여 결과를 재현 버퍼에 저장하고 일부 메트릭을 계산하는 예를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dj4_-77_5ExP",
    "outputId": "d8d73db5-7e11-4eff-d0ac-dc641d15b3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer:\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01609879, -0.03296137, -0.02408002,  0.00126954], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01543957,  0.1624975 , -0.02405462, -0.2989126 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01868952, -0.03227346, -0.03003288, -0.01391207], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01804405, -0.22695212, -0.03031112,  0.2691459 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01350501, -0.42162868, -0.0249282 ,  0.5521165 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.00507243, -0.22616567, -0.01388587,  0.25168496], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.00054912, -0.03084822, -0.00885217, -0.0453453 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([-6.7846020e-05,  1.6439955e-01, -9.7590769e-03, -3.4080797e-01],\n",
      "      dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.00322014, -0.0305822 , -0.01657524, -0.0512184 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.0026085 ,  0.16477345, -0.0175996 , -0.34908453], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.00590397,  0.36014122, -0.02458129, -0.64726484], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.01310679,  0.5555969 , -0.03752659, -0.94758594], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.02421873,  0.3609998 , -0.05647831, -0.666926  ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.03143873,  0.1667069 , -0.06981683, -0.39254716], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.03477287,  0.3627466 , -0.07766777, -0.7063998 ], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.0420278 ,  0.1687817 , -0.09179576, -0.43914092], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(1),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.04540343, -0.02492929, -0.10057858, -0.17674786], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.04490485,  0.17147744, -0.10411354, -0.49938935], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.04833439, -0.02203449, -0.11410133, -0.24124734], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Trajectory(\n",
      "{'action': array(0),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'next_step_type': array(1, dtype=int32),\n",
      " 'observation': array([ 0.0478937 , -0.21535733, -0.11892628,  0.01337825], dtype=float32),\n",
      " 'policy_info': (),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "Average Return:  0.0\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())\n",
    "replay_buffer = []\n",
    "metric = py_metrics.AverageReturnMetric()\n",
    "observers = [replay_buffer.append, metric]\n",
    "driver = py_driver.PyDriver(\n",
    "    env, policy, observers, max_steps=20, max_episodes=1)\n",
    "\n",
    "initial_time_step = env.reset()\n",
    "final_time_step, _ = driver.run(initial_time_step)\n",
    "\n",
    "print('Replay Buffer:')\n",
    "for traj in replay_buffer:\n",
    "  print(traj)\n",
    "\n",
    "print('Average Return: ', metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Yrxg36Ik1x"
   },
   "source": [
    "## TensorFlow 드라이버\n",
    "\n",
    "또한, TensorFlow에는 기능적으로 Python 드라이버와 유사한 드라이버가 있지만, TF environments, TF policies, TF observers 등을 사용합니다. 현재 두 개의 TensorFlow 드라이버, 주어진 수의 (유효한) 환경 단계 후 종료하는 `DynamicStepDriver` 및 주어진 수의 에피소드 후에 종료하는 `DynamicEpisodeDriver`가 있습니다. 동작 중인 DynamicEpisode의 예를 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WC4ba3ObSceA",
    "outputId": "4e69c169-f9e4-4c4f-cee6-f64e42037d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.0089817 , -0.04347435,  0.02990143, -0.02780176]],\n",
      "      dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
      "Number of Steps:  25\n",
      "Number of Episodes:  2\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz5jhHnU0fX1",
    "outputId": "909c44aa-5213-4574-cdff-f1572dc33cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.03743321, -0.01709015, -0.04136974,  0.03487565]],\n",
      "      dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
      "Number of Steps:  57\n",
      "Number of Episodes:  4\n"
     ]
    }
   ],
   "source": [
    "# Continue running from previous state\n",
    "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_drivers_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
