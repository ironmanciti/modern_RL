{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTfA3OUb3Hcx"
   },
   "source": [
    "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rXD5j_4_3Hc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "env = gym.make('CartPole-v1')  \n",
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uciS0nsN3Hc2"
   },
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hyL29xvt3Hc3"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(\n",
    "                        np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(\n",
    "                        np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(\n",
    "                        np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(\n",
    "                        np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(\n",
    "                        np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mBA3O2s3Hc4",
    "outputId": "a8663565-3813-44c1-c76e-12d235bc7b81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([Experience(state=array([-0.00788535,  0.01359877,  0.0407612 ,  0.04364721], dtype=float32), action=0, reward=1.0, next_state=array([-0.00761337, -0.18208325,  0.04163415,  0.34890693], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.00761337, -0.18208325,  0.04163415,  0.34890693], dtype=float32), action=0, reward=1.0, next_state=array([-0.01125504, -0.37777185,  0.04861229,  0.65442234], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.01125504, -0.37777185,  0.04861229,  0.65442234], dtype=float32), action=0, reward=1.0, next_state=array([-0.01881047, -0.57353574,  0.06170073,  0.9620077 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.01881047, -0.57353574,  0.06170073,  0.9620077 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.03028119, -0.3792948 ,  0.08094089,  0.6893292 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.03028119, -0.3792948 ,  0.08094089,  0.6893292 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.03786708, -0.5754412 ,  0.09472747,  1.0063562 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.03786708, -0.5754412 ,  0.09472747,  1.0063562 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.04937591, -0.38170305,  0.1148546 ,  0.74486005], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.04937591, -0.38170305,  0.1148546 ,  0.74486005], dtype=float32), action=0, reward=1.0, next_state=array([-0.05700997, -0.57820684,  0.1297518 ,  1.0713668 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.05700997, -0.57820684,  0.1297518 ,  1.0713668 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.06857411, -0.77478313,  0.15117913,  1.4017928 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.06857411, -0.77478313,  0.15117913,  1.4017928 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.08406977, -0.58182794,  0.17921498,  1.1599387 ], dtype=float32), done=False),\n",
       "       Experience(state=array([-0.08406977, -0.58182794,  0.17921498,  1.1599387 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.09570633, -0.7787735 ,  0.20241377,  1.5030333 ], dtype=float32), done=False)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize replay memory D to capacity N\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 3\n",
    "seed=0\n",
    "\n",
    "replay_memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "s = env.reset()\n",
    "for i in range(10):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    replay_memory.add(s, a, r, s_, done)\n",
    "    s = s_\n",
    "    \n",
    "replay_memory.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBWTiu01BTOB"
   },
   "source": [
    "## Sample random minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8t4tHzM6BO4_",
    "outputId": "52ec13c7-d22d-4cea-b1f5-62e39d5c614f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0494, -0.3817,  0.1149,  0.7449],\n",
       "         [-0.0841, -0.5818,  0.1792,  1.1599],\n",
       "         [-0.0079,  0.0136,  0.0408,  0.0436]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[-0.0570, -0.5782,  0.1298,  1.0714],\n",
       "         [-0.0957, -0.7788,  0.2024,  1.5030],\n",
       "         [-0.0076, -0.1821,  0.0416,  0.3489]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions, rewards, next_states, dones = replay_memory.sample()\n",
    "\n",
    "states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-xKKrqw3Hc5"
   },
   "source": [
    "## Select Action \n",
    "\n",
    "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gbVFe-jz3Hc6"
   },
   "outputs": [],
   "source": [
    "n_inputs = 4  # state feature\n",
    "n_outputs = 2  # action space\n",
    "hidden_layer = 64\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = torch.relu(self.linear1(x))\n",
    "        output = self.linear2(a1)\n",
    "        return output\n",
    "\n",
    "Q = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0fir5Pl3Hc7"
   },
   "source": [
    "- 입력 : 4 개 feature 로 구성된 state \n",
    "- 출력 : 2 개 action values  \n",
    "\n",
    "- greedy action : $max_{a'}Q(s', a';\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ1VsEkd3Hc7",
    "outputId": "e9b88e72-bb7e-4103-8a20-283c6a802f8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0379,  0.0365], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "action_values = Q(torch.tensor(input))\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTXrKPAk3Hc8",
    "outputId": "283be9f8-a151-4530-fa97-6660b06450bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy action\n",
    "action = torch.argmax(action_values).item() \n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dILVsp1p3Hc9"
   },
   "source": [
    "## State-Action Value (q value) from DQN \n",
    "\n",
    "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
    "\n",
    "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NfWJLj63Hc-",
    "outputId": "6086103c-0ce9-4655-a1a0-3d85a0734a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0941,  0.0703],\n",
      "        [-0.1038,  0.0736],\n",
      "        [-0.0204,  0.0640]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.0703, 0.0736, 0.0640]),\n",
      "indices=tensor([1, 1, 1]))\n",
      "\n",
      "tensor([0.0703, 0.0736, 0.0640])\n",
      "tensor([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "action_values = Q(states).detach()\n",
    "\n",
    "print(action_values)\n",
    "print(torch.max(action_values, dim=1))\n",
    "print()\n",
    "\n",
    "values, indices = torch.max(action_values, dim=1)\n",
    "\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSugiYLx3Hc-"
   },
   "source": [
    "## torch.gather\n",
    "\n",
    "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
    "\n",
    "- 첫 번째 인수 인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
    "\n",
    "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
    "\n",
    "```\n",
    "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsSGQnQa3Hc_",
    "outputId": "bd12944d-4e23-4d99-e38b-adb18f3f33b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0494, -0.3817,  0.1149,  0.7449],\n",
       "        [-0.0841, -0.5818,  0.1792,  1.1599],\n",
       "        [-0.0079,  0.0136,  0.0408,  0.0436]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states  # 4개의 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNSRn_CT3Hc_",
    "outputId": "49a75ca0-1192-4cf8-9428-2fb7e258766a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0941,  0.0703],\n",
       "        [-0.1038,  0.0736],\n",
       "        [-0.0204,  0.0640]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = Q(states)\n",
    "q_values  # 2 개의 action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsIuw19X3Hc_",
    "outputId": "e79d6136-de8f-4ccd-a0f8-bc5a360371b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = torch.LongTensor([1, 0, 1]).unsqueeze(1)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXOP_jIq3HdA",
    "outputId": "3c0001f3-c79f-42c3-9dea-a6157fb45340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703],\n",
       "        [-0.1038],\n",
       "        [ 0.0640]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(q_values, 1, action)  #q_value의 axis=1에서 action index 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNvF6oCP3HdA",
    "outputId": "e71ffea2-733c-4cdb-b27b-97762b8cf93f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703],\n",
       "        [-0.1038],\n",
       "        [ 0.0640]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action)   # 위와 동일 operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufB1JYnC3HdB"
   },
   "source": [
    "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
    "\n",
    "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다. \n",
    "\n",
    "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
    "\n",
    "```python\n",
    "    probs = policy_network(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    next_state, reward = env.step(action)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEN8t3cq3HdB"
   },
   "source": [
    "### 방법 1) Categorical(probs) 에서 sampling\n",
    "\n",
    "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRFyebJE3HdB",
    "outputId": "9bfd3d2a-506c-4c3e-bce0-05b1297a669a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax 확률 분포 : tensor([0.2583, 0.2135, 0.1787, 0.3495]), sum = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([4]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "logits = torch.rand(4)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
    "\n",
    "# 각 class 를 sampling 할 상대 확률\n",
    "m = Categorical(probs)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TACqKr0Y3HdC"
   },
   "source": [
    "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHvhhzZO3HdC",
    "outputId": "e80c6733-1ffc-499b-afa6-e5d79576b6e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7663), (1, 6336), (2, 5359), (3, 10642)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = m.sample()\n",
    "    samples.append(a.item())\n",
    "\n",
    "sorted(Counter(samples).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muu7DisY3HdC"
   },
   "source": [
    "### 방법 2) np.random.choice 에서 sampling\n",
    "\n",
    "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewhGZw5w3HdC",
    "outputId": "a677e9d8-a8c3-4e39-f3bf-5cd2719bab89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7787), (1, 6236), (2, 5476), (3, 10501)]\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = np.random.choice(4, p=probs.numpy())\n",
    "    samples.append(a)\n",
    "    \n",
    "print(sorted(Counter(samples).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13-ePWwd3HdD"
   },
   "source": [
    "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dA4SgEC23HdD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgyPm6P-3HdD",
    "outputId": "535f52de-7e7b-4d8d-f382-89cd7155655a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.604476049999999 13.741895 11.8605 8.95 5\n"
     ]
    }
   ],
   "source": [
    "G_0 = 1 + 0.99 * 2 + 0.99**2*3 + 0.99**3*4 + 0.99**4*5\n",
    "G_1 = 2 + 0.99**1*3 + 0.99**2*4 + 0.99**3*5\n",
    "G_2 = 3 + 0.99**1*4 + 0.99**2*5\n",
    "G_3 = 4 + 0.99**1*5\n",
    "G_4 = 5\n",
    "print(G_0, G_1, G_2, G_3, G_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fcFvCy3I3HdD"
   },
   "outputs": [],
   "source": [
    "r = np.array([gamma**i * rewards[i]\n",
    "              for i in range(len(rewards))])\n",
    "# Reverse the array direction for cumsum and then\n",
    "# revert back to the original order\n",
    "r = r[::-1].cumsum()[::-1]\n",
    "# return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkFx4vW13HdD",
    "outputId": "23fb3059-7bd7-4017-baf6-328e164168d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.60447605, 13.741895  , 11.8605    ,  8.95      ,  5.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# episodic task\n",
    "Returns = []\n",
    "G = 0\n",
    "for r in rewards[::-1]:\n",
    "    G = r + gamma * G\n",
    "    Returns.append(G)\n",
    "    \n",
    "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlrV5LBC3HdE",
    "outputId": "bcf11d4c-072a-44b3-d41c-b206e8137305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.60447605 13.741895   11.8605      8.95        5.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.77310184,  2.91052079,  1.02912579, -1.88137421, -5.83137421])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continuing task\n",
    "def discount_rewards(rewards):\n",
    "    Returns = []\n",
    "    G = 0\n",
    "    for r in rewards[::-1]:\n",
    "        G = r + gamma * G\n",
    "        Returns.append(G)\n",
    "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
    "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "    print(Returns)\n",
    "    return Returns - Returns.mean()\n",
    "\n",
    "discount_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simlDDmD3HdE"
   },
   "source": [
    "### REINFORCE 구현을 위한 Score Function\n",
    "\n",
    "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
    "\n",
    "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
    "\n",
    "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
    "\n",
    "\n",
    "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
    "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
    "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhVDiCjk3HdE",
    "outputId": "639ef02f-ed09-4dae-b4ac-f89270436207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7474)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  \n",
    "s = env.reset()\n",
    "\n",
    "#probs = policy_network(state)\n",
    "logits = torch.rand(2)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "m = Categorical(probs)\n",
    "action = m.sample()\n",
    "\n",
    "next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "loss = -m.log_prob(action) * reward\n",
    "#loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZhEg-H53HdE"
   },
   "source": [
    "## Huber Loss\n",
    "\n",
    "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
    "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
    "    - 모든 지점에서 미분이 가능하다.  \n",
    "    - Outlier에 상대적으로 Robust하다.\n",
    "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn8JMIvJ3HdF",
    "outputId": "078551a5-2fd9-4c7a-c036-9e96748705f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
    "target_q = torch.FloatTensor([12,8,10,13,11])\n",
    "loss = F.smooth_l1_loss(curr_q, target_q)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f73e2b_3HdG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "010_basic_operations_for_Function_Approximation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
