{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6aOV15Wc4HP"
   },
   "source": [
    "# Checkpointer and PolicySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3HE5S3wsMEh"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "`tf_agents.utils.common.Checkpointer`는 훈련 상태, 정책 상태, 재생 버퍼 상태를 로컬 스토리지에 저장/로드하는 유틸리티입니다.\n",
    "\n",
    "`tf_agents.policies.policy_saver.PolicySaver`는 정책만 저장/로드하는 도구로 `Checkpointer`보다 가볍습니다. 정책을 생성한 코드에 대한 지식 없이도 `PolicySaver`를 사용하여 모델을 배포할 수 있습니다.\n",
    "\n",
    "이 튜토리얼에서는 DQN을 사용하여 모델을 훈련시킨 다음 `Checkpointer`와 `PolicySaver`를 사용하여 대화식 방식으로 상태와 모델을 저장하고 로드하는 방법을 보여줍니다. 우리는 `PolicySaver`에 대해 TF2.0의 새로운 stored_model 도구 및 형식을 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbTrDrX4dkP_"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opk_cVDYdgct"
   },
   "source": [
    "다음 종속성을 설치하지 않은 경우 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv668dKvZmka"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y -q xvfb ffmpeg python-opengl\n",
    "# !pip install pyglet\n",
    "# !pip install 'imageio==2.4.0'\n",
    "!pip install -q xvfbwrapper\n",
    "!pip install -q tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQMULMo1dCEn"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import IPython\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  files = None\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwIqiLdDCX9Q"
   },
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "import xvfbwrapper\n",
    "xvfbwrapper.Xvfb(1400, 900, 24).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOv_kofIvWnW"
   },
   "source": [
    "## DQN agent\n",
    "이전 colab에서와 같이 DQN 에이전트를 설정할 것입니다. 세부 정보는 이 colab의 핵심 부분이 아니므로 기본적으로 숨겨져 있지만 '코드 표시'를 클릭하여 세부 정보를 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cStmaxredFSW"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "yxFs6QU0dGI_"
   },
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "collect_steps_per_iteration = 100\n",
    "replay_buffer_capacity = 100000\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "log_interval = 5\n",
    "\n",
    "num_eval_episodes = 10\n",
    "eval_interval = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4GR7RDndIOR"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZwK4d-bdI7Z"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AvYRwfkeMvo"
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUrFl83ieOvV"
   },
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=global_step)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8ganoJhdsbn"
   },
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiT1p78HdtSe"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Initial data collection\n",
    "collect_driver.run()\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V8bojrKdupW"
   },
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rDC3leXdvm_"
   },
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "def train_one_iteration():\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_driver.run()\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience)\n",
    "\n",
    "  iteration = agent.train_step_counter.numpy()\n",
    "  print ('iteration: {0} loss: {1}'.format(iteration, train_loss.loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgqVaPnUeDAn"
   },
   "source": [
    "### Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY6w-fcieFDW"
   },
   "outputs": [],
   "source": [
    "def embed_gif(gif_buffer):\n",
    "  \"\"\"Embeds a gif file in the notebook.\"\"\"\n",
    "  tag = '<img src=\"data:image/gif;base64,{0}\"/>'.format(base64.b64encode(gif_buffer).decode())\n",
    "  return IPython.display.HTML(tag)\n",
    "\n",
    "def run_episodes_and_create_video(policy, eval_tf_env, eval_py_env):\n",
    "  num_episodes = 3\n",
    "  frames = []\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = eval_tf_env.reset()\n",
    "    frames.append(eval_py_env.render())\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = eval_tf_env.step(action_step.action)\n",
    "      frames.append(eval_py_env.render())\n",
    "  gif_file = io.BytesIO()\n",
    "  imageio.mimsave(gif_file, frames, format='gif', fps=60)\n",
    "  IPython.display.display(embed_gif(gif_file.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-oA8VYJdFdj"
   },
   "source": [
    "### Generate a video\n",
    "동영상을 생성하여 정책의 성능을 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpmPLXWbdG70"
   },
   "outputs": [],
   "source": [
    "print ('global_step:')\n",
    "print (global_step)\n",
    "run_episodes_and_create_video(agent.policy, eval_env, eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RPLExsxwnOm"
   },
   "source": [
    "## Setup Checkpointer and PolicySaver\n",
    "\n",
    "이제 Checkpointer와 PolicySaver를 사용할 준비가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-iyQJacfQqO"
   },
   "source": [
    "### Checkpointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DzCJZ-6YYbX"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKpWNZM4WE8d"
   },
   "source": [
    "### Policy Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mDZ_YMUWEY9"
   },
   "outputs": [],
   "source": [
    "policy_dir = os.path.join(tempdir, 'policy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OnANb1Idx8-"
   },
   "source": [
    "### Train one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql_D1iq8dl0X"
   },
   "outputs": [],
   "source": [
    "print('Training one iteration....')\n",
    "train_one_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSChNSQPlySb"
   },
   "source": [
    "### Save to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usDm_Wpsl0bu"
   },
   "outputs": [],
   "source": [
    "train_checkpointer.save(global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTQUrKgihuic"
   },
   "source": [
    "### Restore checkpoint\n",
    "\n",
    "이것이 작동하려면 체크포인트가 생성되었을 때와 같은 방식으로 전체 객체 세트를 다시 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6l3EB-Yhwmz"
   },
   "outputs": [],
   "source": [
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb8_MSE2XjRp"
   },
   "source": [
    "또한 정책을 저장하고 위치로 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xHz09WCWjwA"
   },
   "outputs": [],
   "source": [
    "tf_policy_saver.save(policy_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz-xScbuh4Vo"
   },
   "source": [
    "정책을 만드는 데 사용된 에이전트 또는 네트워크에 대한 지식 없이도 정책을 로드할 수 있습니다. 이렇게 하면 정책을 훨씬 쉽게 배포할 수 있습니다.\n",
    "\n",
    "저장된 정책을 로드하고 작동 방식을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6T5KLTMh9ZB"
   },
   "outputs": [],
   "source": [
    "saved_policy = tf.saved_model.load(policy_dir)\n",
    "run_episodes_and_create_video(saved_policy, eval_env, eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpE0KKfqjc0c"
   },
   "source": [
    "## Export and import\n",
    "colab의 나머지 부분은 체크포인터 및 정책 디렉터리를 내보내거나 가져오는 데 도움이 될 것입니다. 이를 통해 나중에 학습을 계속하고 다시 학습할 필요 없이 모델을 배포할 수 있습니다.\n",
    "\n",
    "이제 `Train one iteration`으로 돌아가서 나중에 차이점을 이해할 수 있도록 몇 번 더 훈련할 수 있습니다. 약간 더 나은 결과를 보기 시작하면 아래를 계속하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd5Cj7DVjfH4"
   },
   "outputs": [],
   "source": [
    "#Create zip file and upload zip file (double-click to see the code)\n",
    "def create_zip_file(dirname, base_filename):\n",
    "  return shutil.make_archive(base_filename, 'zip', dirname)\n",
    "\n",
    "def upload_and_unzip_file_to(dirname):\n",
    "  if files is None:\n",
    "    return\n",
    "  uploaded = files.upload()\n",
    "  for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "        name=fn, length=len(uploaded[fn])))\n",
    "    shutil.rmtree(dirname)\n",
    "    zip_files = zipfile.ZipFile(io.BytesIO(uploaded[fn]), 'r')\n",
    "    zip_files.extractall(dirname)\n",
    "    zip_files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgyy29doHCmL"
   },
   "source": [
    "checkpoint 디렉터리로 부터 압축 파일을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhR8NeWzF4fe"
   },
   "outputs": [],
   "source": [
    "train_checkpointer.save(global_step)\n",
    "checkpoint_zip_filename = create_zip_file(checkpoint_dir, os.path.join(tempdir, 'exported_cp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGEpntTocd2u"
   },
   "source": [
    "Download the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upFxb5k8b4MC"
   },
   "outputs": [],
   "source": [
    "if files is not None:\n",
    "  files.download(checkpoint_zip_filename) # try again if this fails: https://github.com/googlecolab/colabtools/issues/469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRaZMrn5jLmE"
   },
   "source": [
    "일정 시간(10~15회) 훈련 후 체크포인트 zip 파일을 다운로드하고 \"런타임 > 다시 시작 및 모두 실행\"으로 이동하여 훈련을 재설정하고 이 셀로 돌아옵니다. 이제 다운로드한 zip 파일을 업로드하고 교육을 계속할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kg-bKgMsF-H_"
   },
   "outputs": [],
   "source": [
    "upload_and_unzip_file_to(checkpoint_dir)\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrNax5Zk3vF"
   },
   "source": [
    "체크포인트 디렉토리를 업로드했으면  `Train one iteration`으로 돌아가서 학습을 계속하거나  `Generate a video` 로 돌아가 로드된 정책의 성능을 확인하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAkvVZ-NeN2j"
   },
   "source": [
    "다른 방법으로는 policy (model)을 저장하고 복원할 수 있습니다.\n",
    "checkpointer와 달리 교육을 계속할 수는 없지만 모델을 배포할 수는 있습니다. 다운로드한 파일은 checkpointer 보다 훨씬 작습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7qMn6D8eiIA"
   },
   "outputs": [],
   "source": [
    "tf_policy_saver.save(policy_dir)\n",
    "policy_zip_filename = create_zip_file(policy_dir, os.path.join(tempdir, 'exported_policy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrGvCEXwerJj"
   },
   "outputs": [],
   "source": [
    "if files is not None:\n",
    "  files.download(policy_zip_filename) # try again if this fails: https://github.com/googlecolab/colabtools/issues/469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyC_O_gsgSi5"
   },
   "source": [
    "다운로드한 정책 디렉터리(exported_policy.zip)를 업로드하고 저장되었던 정책이 어떻게 수행되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgWLimRlXy5z"
   },
   "outputs": [],
   "source": [
    "upload_and_unzip_file_to(policy_dir)\n",
    "saved_policy = tf.saved_model.load(policy_dir)\n",
    "run_episodes_and_create_video(saved_policy, eval_env, eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSehXThTm4af"
   },
   "source": [
    "## SavedModelPyTFEagerPolicy\n",
    "\n",
    "TF policy를 사용하지 않으려면 `py_tf_eager_policy.SavedModelPyTFEAgerPolicy`를 사용하여 Python 환경과 함께 stored_model을 직접 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUC5XuLf1jF7"
   },
   "outputs": [],
   "source": [
    "eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
    "    policy_dir, eval_py_env.time_step_spec(), eval_py_env.action_spec())\n",
    "\n",
    "# Note that we're passing eval_py_env not eval_env.\n",
    "run_episodes_and_create_video(eager_py_policy, eval_py_env, eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fvWqfJg00ww"
   },
   "source": [
    "## Convert policy to TFLite\n",
    "\n",
    "See [TensorFlow Lite Inference](https://tensorflow.org/lite/guide/inference) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9zonVBJ0z46"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(policy_dir, signature_keys=[\"action\"])\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_policy = converter.convert()\n",
    "with open(os.path.join(tempdir, 'policy.tflite'), 'wb') as f:\n",
    "  f.write(tflite_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsi3V9QdxJUu"
   },
   "source": [
    "### Run inference on TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GeUSWyZxMlN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "interpreter = tf.lite.Interpreter(os.path.join(tempdir, 'policy.tflite'))\n",
    "\n",
    "policy_runner = interpreter.get_signature_runner()\n",
    "print(policy_runner._inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVVrdTbRxnOC"
   },
   "outputs": [],
   "source": [
    "policy_runner(**{\n",
    "    '0/discount':tf.constant(0.0),\n",
    "    '0/observation':tf.zeros([1,4]),\n",
    "    '0/reward':tf.constant(0.0),\n",
    "    '0/step_type':tf.constant(0)})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF-Agent Checkpointer / PolicySaver Colab",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
