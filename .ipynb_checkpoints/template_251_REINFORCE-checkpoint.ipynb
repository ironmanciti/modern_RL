{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b961f69b",
   "metadata": {
    "id": "b961f69b"
   },
   "source": [
    "# 251. REINFORCE Algorithm\n",
    "\n",
    "- Monte-Carlo method를 통해 구한 episodic sample 의 estimated return을 이용하여 policy parameter 𝜃를 update해 나가는 기법\n",
    "\n",
    "- REINFORCE 갱신 규칙\n",
    "\n",
    "$$\\Delta\\theta_t = \\alpha\\nabla_\\theta\\log{\\pi_\\theta}(s, a)G_t$$\n",
    "\n",
    "- 따라서, Loss function 은 \n",
    "$$-G_t\\log{\\pi_\\theta}(s, a)$$\n",
    "\n",
    "```\n",
    "                log_prob = torch.log(pi(state_tensor))\n",
    "                selected_log_probs = reward_tensor * \\\n",
    "                        torch.gather(log_prob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "                loss = -1 * selected_log_probs.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qCER-H6cO7Rh",
   "metadata": {
    "id": "qCER-H6cO7Rh"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*4RncZNj1ij5A5eMJpexhrw.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814aa4c",
   "metadata": {
    "id": "6814aa4c"
   },
   "source": [
    "### 환경 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cc627",
   "metadata": {
    "id": "420cc627"
   },
   "source": [
    "### Policy Network 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce56db",
   "metadata": {
    "id": "0cce56db"
   },
   "source": [
    "### hyper-parameters 설정, reward 계산 도우미 함수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9655c",
   "metadata": {
    "id": "48d9655c"
   },
   "source": [
    "### main algorithm 작성 / Train\n",
    "- `CartPole-v0`의 경우 train 약 10 분 소요\n",
    "- `LunarLander-v2` 의 경우 train 시간 약 1 시간 소요\n",
    "- 시간 부족할 경우 이미 훈련된  `251_REINFORCE_LunarLander-V2.pth` 를 load 하여 시각화 Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "batch_rewards = []\n",
    "batch_actions = []\n",
    "batch_states = []\n",
    "batch_counter = 1\n",
    "\n",
    "# While episode n < N do: (training의 안정성 제고)\n",
    "for episode in range(N):\n",
    "   \n",
    "\n",
    "\n",
    "    # for K batches do:\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    done = False\n",
    "    # Generate an episode s0, a0, r0,...st,at,rt following policy pi(a|s,theta)\n",
    "    while not done:\n",
    "       \n",
    "        \n",
    "#         if rendering and (episode > N * 0.98):\n",
    "#             env.render()\n",
    "        \n",
    "        if done:\n",
    "            # for each step in the eposide(t), discount reward do:\n",
    "            # G_t = sum from t=1 to t=T {gamma^t * R_t}\n",
    "            \n",
    "        \n",
    "            # If batch is complete, update network\n",
    "            if batch_counter == batch_size:    \n",
    "                \n",
    "                \n",
    "                # Calculate policy loss for all episides in the batch \n",
    "                # L(theta) = -1/m sum(ln(G_t pi(a|s,theta)))))\n",
    "                \n",
    "    \n",
    "                # Update the policy: \n",
    "                # theta <- theta + alpha * grad[L(theat)]\n",
    "                \n",
    "\n",
    "                batch_rewards = []\n",
    "                batch_actions = []\n",
    "                batch_states = []\n",
    "                batch_counter = 1\n",
    "                \n",
    "    if episode % 100 == 0:\n",
    "        avg_score = np.mean(total_rewards[-100:])\n",
    "        print(f'episode {episode},  최근 100 episode 평균 reward {avg_score: .2f}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840559f",
   "metadata": {
    "id": "8840559f"
   },
   "source": [
    "### reward 변화 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qxsHHDlOW82",
   "metadata": {
    "id": "4qxsHHDlOW82"
   },
   "source": [
    "### 이미 학습되어 저장된 model load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d550b",
   "metadata": {
    "id": "2f5d550b"
   },
   "source": [
    "### Animate it with Video"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "251_REINFORCE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
