{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b961f69b",
   "metadata": {
    "id": "b961f69b"
   },
   "source": [
    "# 251. REINFORCE Algorithm\n",
    "\n",
    "- Monte-Carlo methodë¥¼ í†µí•´ êµ¬í•œ episodic sample ì˜ estimated returnì„ ì´ìš©í•˜ì—¬ policy parameter ğœƒë¥¼ updateí•´ ë‚˜ê°€ëŠ” ê¸°ë²•\n",
    "\n",
    "- REINFORCE ê°±ì‹  ê·œì¹™\n",
    "\n",
    "$$\\Delta\\theta_t = \\alpha\\nabla_\\theta\\log{\\pi_\\theta}(s, a)G_t$$\n",
    "\n",
    "- ë”°ë¼ì„œ, Loss function ì€ \n",
    "$$-G_t\\log{\\pi_\\theta}(s, a)$$\n",
    "\n",
    "```\n",
    "                log_prob = torch.log(pi(state_tensor))\n",
    "                selected_log_probs = reward_tensor * \\\n",
    "                        torch.gather(log_prob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "                loss = -1 * selected_log_probs.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qCER-H6cO7Rh",
   "metadata": {
    "id": "qCER-H6cO7Rh"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*4RncZNj1ij5A5eMJpexhrw.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814aa4c",
   "metadata": {
    "id": "6814aa4c"
   },
   "source": [
    "### í™˜ê²½ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cc627",
   "metadata": {
    "id": "420cc627"
   },
   "source": [
    "### Policy Network ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce56db",
   "metadata": {
    "id": "0cce56db"
   },
   "source": [
    "### hyper-parameters ì„¤ì •, reward ê³„ì‚° ë„ìš°ë¯¸ í•¨ìˆ˜ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9655c",
   "metadata": {
    "id": "48d9655c"
   },
   "source": [
    "### main algorithm ì‘ì„± / Train\n",
    "- `CartPole-v0`ì˜ ê²½ìš° train ì•½ 10 ë¶„ ì†Œìš”\n",
    "- `LunarLander-v2` ì˜ ê²½ìš° train ì‹œê°„ ì•½ 1 ì‹œê°„ ì†Œìš”\n",
    "- ì‹œê°„ ë¶€ì¡±í•  ê²½ìš° ì´ë¯¸ í›ˆë ¨ëœ  `251_REINFORCE_LunarLander-V2.pth` ë¥¼ load í•˜ì—¬ ì‹œê°í™” Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "batch_rewards = []\n",
    "batch_actions = []\n",
    "batch_states = []\n",
    "batch_counter = 1\n",
    "\n",
    "# While episode n < N do: (trainingì˜ ì•ˆì •ì„± ì œê³ )\n",
    "for episode in range(N):\n",
    "   \n",
    "\n",
    "\n",
    "    # for K batches do:\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    done = False\n",
    "    # Generate an episode s0, a0, r0,...st,at,rt following policy pi(a|s,theta)\n",
    "    while not done:\n",
    "       \n",
    "        \n",
    "#         if rendering and (episode > N * 0.98):\n",
    "#             env.render()\n",
    "        \n",
    "        if done:\n",
    "            # for each step in the eposide(t), discount reward do:\n",
    "            # G_t = sum from t=1 to t=T {gamma^t * R_t}\n",
    "            \n",
    "        \n",
    "            # If batch is complete, update network\n",
    "            if batch_counter == batch_size:    \n",
    "                \n",
    "                \n",
    "                # Calculate policy loss for all episides in the batch \n",
    "                # L(theta) = -1/m sum(ln(G_t pi(a|s,theta)))))\n",
    "                \n",
    "    \n",
    "                # Update the policy: \n",
    "                # theta <- theta + alpha * grad[L(theat)]\n",
    "                \n",
    "\n",
    "                batch_rewards = []\n",
    "                batch_actions = []\n",
    "                batch_states = []\n",
    "                batch_counter = 1\n",
    "                \n",
    "    if episode % 100 == 0:\n",
    "        avg_score = np.mean(total_rewards[-100:])\n",
    "        print(f'episode {episode},  ìµœê·¼ 100 episode í‰ê·  reward {avg_score: .2f}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840559f",
   "metadata": {
    "id": "8840559f"
   },
   "source": [
    "### reward ë³€í™” ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qxsHHDlOW82",
   "metadata": {
    "id": "4qxsHHDlOW82"
   },
   "source": [
    "### ì´ë¯¸ í•™ìŠµë˜ì–´ ì €ì¥ëœ model load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d550b",
   "metadata": {
    "id": "2f5d550b"
   },
   "source": [
    "### Animate it with Video"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "251_REINFORCE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
